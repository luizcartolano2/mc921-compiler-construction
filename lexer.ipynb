{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DÃºvidas\n",
    "1. Assign ou Equals?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'tokrules' from '/Users/luizeduardocartolano/OneDrive/DUDU/Unicamp/IC/mc921/mc921-compiler-construction/tokrules.py'>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ply.lex as lex\n",
    "import tokrules\n",
    "import importlib\n",
    "\n",
    "importlib.reload(tokrules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Build Lexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: No t_error rule is defined\n"
     ]
    }
   ],
   "source": [
    "# build the lexer\n",
    "lexer = lex.lex(module=tokrules)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 4: Give input to the lexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_lex = \"\"\"/* comment */\n",
    "int j = 3;\n",
    "    int main () {\n",
    "        int i = j;\n",
    "        int k = 3;\n",
    "        int p = 2 * j;\n",
    "        assert p == 2 * i;\n",
    "    }\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# give the lexer some input\n",
    "lexer.input(input_lex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 5: Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LexToken(INT,'int',2,14)\n",
      "LexToken(ID,'j',2,18)\n",
      "LexToken(EQUALS,'=',2,20)\n",
      "LexToken(ICONST,3,2,22)\n",
      "LexToken(SEMI,';',2,23)\n",
      "LexToken(INT,'int',3,29)\n",
      "LexToken(ID,'main',3,33)\n",
      "LexToken(LPAREN,'(',3,38)\n",
      "LexToken(RPAREN,')',3,39)\n",
      "LexToken(LBRACE,'{',3,41)\n",
      "LexToken(INT,'int',4,51)\n",
      "LexToken(ID,'i',4,55)\n",
      "LexToken(EQUALS,'=',4,57)\n",
      "LexToken(ID,'j',4,59)\n",
      "LexToken(SEMI,';',4,60)\n",
      "LexToken(INT,'int',5,70)\n",
      "LexToken(ID,'k',5,74)\n",
      "LexToken(EQUALS,'=',5,76)\n",
      "LexToken(ICONST,3,5,78)\n",
      "LexToken(SEMI,';',5,79)\n",
      "LexToken(INT,'int',6,89)\n",
      "LexToken(ID,'p',6,93)\n",
      "LexToken(EQUALS,'=',6,95)\n",
      "LexToken(ICONST,2,6,97)\n",
      "LexToken(TIMES,'*',6,99)\n",
      "LexToken(ID,'j',6,101)\n",
      "LexToken(SEMI,';',6,102)\n",
      "LexToken(ASSERT,'assert',7,112)\n",
      "LexToken(ID,'p',7,119)\n",
      "LexToken(EQ,'==',7,121)\n",
      "LexToken(ICONST,2,7,124)\n",
      "LexToken(TIMES,'*',7,126)\n",
      "LexToken(ID,'i',7,128)\n",
      "LexToken(SEMI,';',7,129)\n",
      "LexToken(RBRACE,'}',8,135)\n"
     ]
    }
   ],
   "source": [
    "# Tokenize\n",
    "while True:\n",
    "    tok = lexer.token()\n",
    "    # No more input\n",
    "    if not tok: \n",
    "        break      \n",
    "    print(tok)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
